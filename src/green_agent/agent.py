import dotenv
import json
import time
from a2a.server.tasks import TaskUpdater
from a2a.types import (
    Message,
    TaskState,
    Part,
    TextPart,
    SendMessageSuccessResponse,
    Message,
)
from a2a.utils import get_message_text, new_agent_text_message, get_text_parts
from src.util.a2a_comm import send_message
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import Any, Dict, List, Optional

dotenv.load_dotenv()


def read_from_jsonl(file_path):
    data = []
    with open(file_path, "r", encoding="utf-8") as file:
        for line in file:
            data.append(json.loads(line.strip()))
    return data


@dataclass
class BenchmarkResult:
    problem_name: str
    problem_id: str
    success: bool
    time_used_sec: float
    is_error: bool
    error_message: Optional[str] = None
    stdout: Optional[str] = None
    white_agent_text: Optional[str] = None
    file_path: Optional[str] = None
    cli_args: Optional[str] = None


class Agent:
    def __init__(self, white_agent_url, mcp_client, max_num_steps=1):
        self.white_agent_url = white_agent_url
        self.mcp_client = mcp_client
        self.max_num_steps = max_num_steps
        self.metrics = {}
        # Initialize state here

    async def run(self, message: Message, updater: TaskUpdater) -> None:
        """Green agent implementation - manages assessment and evaluation.

        This Green agent distributes test tasks to participant agents and collects their response. No environment interaction or multiple steps for now.

        Args:
            message: The incoming message
            updater: Report progress (update_status) and results (add_artifact)

        Use send_message(message, url) to call participant agents.
        """
        results: List[BenchmarkResult] = []
        summary: Dict[str, Any] = {
            "total": 0,
            "success_count": 0,
            "failure_count": 0,
            "avg_time_sec": None,
        }

        input_text = get_message_text(message)
        data_file_path = Path("./data/problems_test.jsonl")
        test_data = read_from_jsonl(data_file_path)

        for idx, data in enumerate(test_data, start=1):
            timestamp_started = time.time()
            pname = data["problem_name"]
            pid = data["problem_id"]
            pdesc = data["problem_description"]

            await updater.update_status(
                TaskState.working,
                new_agent_text_message(f"[{idx}/{len(test_data)}] Running {pname}..."),
            )
            br = BenchmarkResult(
                problem_name=pname,
                problem_id=pid,
                success=False,
                time_used_sec=0.0,
                is_error=False,
            )
            try:
                print(
                    f"@@@ Green agent: Sending message to white agent... -->\n{pdesc}"
                )

                # (1) send task description to white agent
                white_agent_response = await send_message(
                    self.white_agent_url,
                    pdesc,
                    context_id=pname,
                )
                res_root = white_agent_response.root
                assert isinstance(res_root, SendMessageSuccessResponse)
                res_result = res_root.result
                assert isinstance(
                    res_result, Message
                )  # though, a robust design should also support Task
                text_parts = get_text_parts(res_result.parts)
                assert (
                    len(text_parts) == 1
                ), "Expecting exactly one text part from the white agent"
                white_text = text_parts[0]
                # br.white_agent_text = white_text

                # (2) parse response
                print(f"@@@ White agent response:\n{white_text}")
                lines = white_text.strip().splitlines()
                file_path = lines[1].strip()
                cli_args = lines[2].strip()
                # br.file_path = file_path
                # br.cli_args = cli_args

                # (3) build and run
                print(
                    f"@@@ Green agent: Compile and run the code generated by white agent..."
                )
                await self.mcp_client.run_bash_command("upload_file", file_path)
                await self.mcp_client.run_bash_command("make", pname)
                run_result = await self.mcp_client.run_bash_command(
                    f"./{pname}", cli_args
                )
                print(run_result.content[0].text)  # use LLM as a judge

                br.is_error = run_result.isError
                br.success = not br.is_error
            except Exception as e:
                br.is_error = True
                br.success = False
                br.error_message = f"{type(e).__name__}: {e}"
            finally:
                br.time_used_sec = time.time() - timestamp_started
                results.append(br)
                # Update rolling summary
                summary["total"] += 1
                if br.success:
                    summary["success_count"] += 1
                else:
                    summary["failure_count"] += 1
                # result_emoji = "✅" if br.success else "❌"
                # Optional: per-case artifact (useful for debugging)
                await updater.add_artifact(
                    name=f"benchmark_result_{pname}.json",
                    parts=[TextPart(text=json.dumps(asdict(br), indent=2))],
                )
        # Final summary artifact
        times = [r.time_used_sec for r in results]
        summary["avg_time_sec"] = (sum(times) / len(times)) if times else None

        await updater.add_artifact(
            name="benchmark_summary.json",
            parts=[
                TextPart(
                    text=json.dumps(
                        {"summary": summary, "results": [asdict(r) for r in results]},
                        indent=2,
                    )
                )
            ],
            metadata=summary,
        )
        await updater.update_status(
            TaskState.completed,
            new_agent_text_message(
                f"Done. {summary['success_count']}/{summary['total']} succeeded."
            ),
        )
