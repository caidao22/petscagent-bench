# Evaluation Configuration

evaluation:
  # Enable/disable evaluation phases
  enable_gates: true
  enable_metrics: true
  enable_quality: true
  
  # LLM settings for quality evaluators
  llm:
    model: "openai/gpt-5.2"
    temperature: 0.3
    max_concurrent_calls: 3  # Rate limiting
  
  # Performance settings
  parallel_evaluation: true
  
  # LLM Thresholds
  thresholds:
    min_llm_confidence: 0.7

# Scoring configuration
scoring:
  # Category weights (must sum to 1.0)
  weights:
    correctness: 0.35   # Numerical accuracy, semantic correctness
    performance: 0.15   # Execution time
    code_quality: 0.15  # Readability, style, documentation, modularity
    algorithm: 0.15     # Algorithm choice, solver, discretization
    petsc: 0.10         # PETSc best practices, error handling, etc.
    semantic: 0.10      # BCs, ICs, physics preservation
  
  # Tier thresholds (0-100 scale)
  tiers:
    gold: 85
    silver: 70
    bronze: 50

# Evaluator-specific configurations
# Code quality evaluators
readability:
  use_llm: true  # Set to false to use static analysis instead

code_style:
  use_llm: true

documentation:
  use_llm: true

modularity:
  use_llm: true

# Metric configurations
numerical_accuracy:
  error_tolerance: 1.0e-3
  error_threshold: 1.0e-3

execution_time:
  excellent_time_sec: 1.0
  good_time_sec: 5.0
  acceptable_time_sec: 15.0
  max_time_sec: 60.0
  max_slowdown_factor: 2.0
