# Evaluation Configuration

evaluation:
  # Enable/disable evaluation phases
  enable_gates: true
  enable_metrics: true
  enable_quality: true
  
  # LLM settings for quality evaluators and agents
  llm:
    model: "gpt51"   # LLM for green agent (quality evaluation)
    purple_model: null            # LLM for purple agent (code generation); null = use model
    api_base_url: "https://apps-dev.inside.anl.gov/argoapi/v1"            # API base URL (e.g., "https://api.openai.com/v1"); null = use default
                                  # For OpenAI-compatible APIs (ends with /v1), use base URL:
                                  # Example: "https://apps-dev.inside.anl.gov/argoapi/v1"
                                  # For custom APIs, use base URL without endpoint path
    temperature: 0                # Set to 0 only for reproducibility
    max_concurrent_calls: 3       # Rate limiting
  
  # Performance settings
  parallel_evaluation: true

# Scoring configuration
scoring:
  # Category weights (must sum to 1.0)
  weights:
    correctness: 0.35   # Numerical accuracy, semantic correctness
    performance: 0.15   # Execution time
    code_quality: 0.15  # Readability, style, documentation, modularity
    algorithm: 0.15     # Algorithm choice, solver, discretization
    petsc: 0.10         # PETSc best practices, error handling, etc.
    semantic: 0.10      # BCs, ICs, physics preservation
  
  # Tier thresholds (0-100 scale)
  tiers:
    gold: 85
    silver: 70
    bronze: 50

# Evaluator-specific configurations
# Code quality evaluators
readability:
  use_llm: true  # Set to false to use static analysis instead

code_style:
  use_llm: true

documentation:
  use_llm: true

modularity:
  use_llm: true

# Metric configurations
numerical_accuracy:
  error_tolerance: 1.0e-3
  error_threshold: 1.0e-3

execution_time:
  excellent_time_sec: 1.0
  good_time_sec: 5.0
  acceptable_time_sec: 15.0
  max_time_sec: 60.0
  max_slowdown_factor: 2.0
