# PETSc Code Evaluation System - Implementation Summary (Updated)

## Overview

A comprehensive evaluation framework for assessing generated PETSc code with **14 evaluators** across 3 evaluation types, **fully integrated** into the Green Agent benchmarking pipeline.

## Architecture

### Three-Tier Evaluation Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EVALUATION PIPELINE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Phase 1: GATES (Must Pass)                                â”‚
â”‚  â”œâ”€â”€ Compilation          (deterministic)                  â”‚
â”‚  â”œâ”€â”€ Execution            (deterministic)                  â”‚
â”‚  â”œâ”€â”€ Memory Safety        (deterministic)                  â”‚
â”‚  â””â”€â”€ API Usage            (deterministic)                  â”‚
â”‚                                                             â”‚
â”‚  Phase 2: METRICS (Measurements)                           â”‚
â”‚  â”œâ”€â”€ Numerical Accuracy   (deterministic)                  â”‚
â”‚  â””â”€â”€ Execution Time       (deterministic)                  â”‚
â”‚                                                             â”‚
â”‚  Phase 3: QUALITY (Assessments)                            â”‚
â”‚  â”œâ”€â”€ Code Quality         (LLM or static)                  â”‚
â”‚  â”‚   â”œâ”€â”€ Readability                                       â”‚
â”‚  â”‚   â”œâ”€â”€ Code Style                                        â”‚
â”‚  â”‚   â””â”€â”€ Documentation                                     â”‚
â”‚  â”œâ”€â”€ Algorithm Quality    (LLM)                            â”‚
â”‚  â”‚   â”œâ”€â”€ Algorithm Appropriateness                         â”‚
â”‚  â”‚   â””â”€â”€ Solver Choice                                     â”‚
â”‚  â””â”€â”€ PETSc Quality        (mixed)                          â”‚
â”‚      â”œâ”€â”€ Best Practices   (LLM)                            â”‚
â”‚      â”œâ”€â”€ Error Handling   (deterministic)                  â”‚
â”‚      â””â”€â”€ Parallel Aware   (deterministic)                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   METRICS AGGREGATOR                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Category Scores (0-100):                                  â”‚
â”‚  â”œâ”€â”€ Correctness  (35% weight)                            â”‚
â”‚  â”œâ”€â”€ Performance  (15% weight)                            â”‚
â”‚  â”œâ”€â”€ Code Quality (15% weight)                            â”‚
â”‚  â”œâ”€â”€ Algorithm    (15% weight)                            â”‚
â”‚  â”œâ”€â”€ PETSc Usage  (10% weight)                            â”‚
â”‚  â””â”€â”€ Semantic     (10% weight)                            â”‚
â”‚                                                             â”‚
â”‚  Composite Score = Î£(category Ã— weight)                   â”‚
â”‚                                                             â”‚
â”‚  Tier Assignment:                                          â”‚
â”‚  â”œâ”€â”€ GOLD   (â‰¥85)                                         â”‚
â”‚  â”œâ”€â”€ SILVER (â‰¥70)                                         â”‚
â”‚  â”œâ”€â”€ BRONZE (â‰¥50)                                         â”‚
â”‚  â””â”€â”€ FAIL   (<50 or gates failed)                         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Complete Metrics List (14 Total)

### 1. Gates (4) - Binary Pass/Fail

| Metric | What | Method | Critical |
|--------|------|--------|----------|
| **Compilation** | Code compiles successfully | Deterministic | âœ… |
| **Execution** | Runs without crash | Deterministic | âœ… |
| **Memory Safety** | No leaks/errors (Valgrind) | Deterministic | âœ… |
| **API Usage** | PetscInit/Finalize present | Static Analysis | âœ… |

**If ANY gate fails â†’ Overall score = 0 (FAIL)**

### 2. Metrics (2) - Continuous Measurements

| Metric | Raw Value | Normalized Score | Method |
|--------|-----------|------------------|--------|
| **Numerical Accuracy** | Error norm | exp(-error/tol) | Deterministic |
| **Execution Time** | Seconds | baseline/actual | Deterministic |

### 3. Quality (8) - Subjective Assessments

#### Code Quality (3)
| Metric | Assessment | Default Method | Configurable |
|--------|------------|----------------|---------------|
| **Readability** | Variable names, structure | LLM | âœ… Static option |
| **Code Style** | PETSc/C conventions | LLM | âœ… Static option |
| **Documentation** | Comments, clarity | LLM | âœ… Static option |

#### Algorithm Quality (2)
| Metric | Assessment | Method |
|--------|------------|--------|
| **Algorithm Appropriateness** | Suitable approach | LLM |
| **Solver Choice** | KSP/SNES type | LLM |

#### PETSc Quality (3)
| Metric | Assessment | Method |
|--------|------------|--------|
| **Best Practices** | CLI options, viewers | LLM |
| **Error Handling** | CHKERRQ usage | Deterministic |
| **Parallel Awareness** | MPI-aware code | Deterministic |

## Implementation Structure

```
petscagent_bench/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ evaluators/
â”‚   â”‚   â”œâ”€â”€ __init__.py                # Exports
â”‚   â”‚   â”œâ”€â”€ base.py                    # Base classes, enums
â”‚   â”‚   â”œâ”€â”€ pipeline.py                # Orchestration
â”‚   â”‚   â”œâ”€â”€ README.md                  # Documentation
â”‚   â”‚   â”œâ”€â”€ gates/                     # 4 gate evaluators
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ compilation_gate.py
â”‚   â”‚   â”‚   â”œâ”€â”€ execution_gate.py
â”‚   â”‚   â”‚   â”œâ”€â”€ memory_safety_gate.py
â”‚   â”‚   â”‚   â””â”€â”€ api_usage_gate.py
â”‚   â”‚   â”œâ”€â”€ metrics/                   # 2 metric evaluators
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ numerical_accuracy.py
â”‚   â”‚   â”‚   â””â”€â”€ execution_time.py
â”‚   â”‚   â””â”€â”€ quality/                   # 8 quality evaluators
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ code_quality/          # 3 evaluators
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ readability.py
â”‚   â”‚       â”‚   â”œâ”€â”€ code_style.py
â”‚   â”‚       â”‚   â””â”€â”€ documentation.py
â”‚   â”‚       â”œâ”€â”€ algorithm_quality/     # 2 evaluators
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ algorithm_appropriateness.py
â”‚   â”‚       â”‚   â””â”€â”€ solver_choice.py
â”‚   â”‚       â””â”€â”€ petsc_quality/         # 3 evaluators
â”‚   â”‚           â”œâ”€â”€ __init__.py
â”‚   â”‚           â”œâ”€â”€ best_practices.py
â”‚   â”‚           â”œâ”€â”€ error_handling.py
â”‚   â”‚           â””â”€â”€ parallel_awareness.py
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ types.py                   # Data structures
â”‚   â”‚   â””â”€â”€ aggregation.py             # Scoring logic
â”‚   â”œâ”€â”€ util/
â”‚   â”‚   â””â”€â”€ llm_client.py              # OpenAI wrapper
â”‚   â””â”€â”€ green_agent/
â”‚       â””â”€â”€ agent.py                   # âœ… INTEGRATED
â”œâ”€â”€ config/
â”‚   â””â”€â”€ evaluation_config.yaml         # Configuration
â””â”€â”€ examples/
    â””â”€â”€ evaluation_example.py          # Usage example
```

## Key Design Decisions

### 1. Why 3 Types (Gates/Metrics/Quality)?

**Semantic types instead of method-based classification:**

- **Gates**: Must-pass requirements (compilation, execution, memory safety, API usage)
- **Metrics**: Objective measurements (time, error)
- **Quality**: Subjective assessments (readability, appropriateness)

This separates **WHAT** we evaluate from **HOW** we evaluate it.

### 2. Why Both Deterministic and LLM?

| Use Deterministic When | Use LLM When |
|------------------------|---------------|
| âœ… Objective facts (compiles? runs?) | âœ… Subjective quality (readable?) |
| âœ… Measurable (time, error) | âœ… Semantic understanding (correct approach?) |
| âœ… Ground truth available | âœ… No reference solution |
| âœ… Fast & free | âœ… Complex reasoning needed |

**Rule**: If you can measure it objectively, don't use LLM.

### 3. Scoring Formula

```python
if not all_gates_passed:
    score = 0
else:
    score = (
        0.35 Ã— correctness +      # Numerical accuracy
        0.15 Ã— performance +       # Execution time
        0.15 Ã— code_quality +      # Readability, style, docs
        0.15 Ã— algorithm +         # Algorithm, solver
        0.10 Ã— petsc +             # Best practices
        0.10 Ã— semantic            # (Reserved for future use)
    )
```

## Full Green Agent Integration

The evaluation system is **fully integrated** into the Green Agent's benchmarking pipeline:

### Enhanced BenchmarkResult

```python
@dataclass
class BenchmarkResult:
    problem_name: str
    problem_id: str
    runs: bool
    time_used_sec: float
    compiles: bool
    stdout: Optional[str] = None
    stderr: Optional[str] = None
    cli_args: Optional[str] = None
    
    # Evaluation fields (NEW)
    composite_score: Optional[float] = None  # 0-100
    tier: Optional[str] = None  # GOLD/SILVER/BRONZE/FAIL
    category_scores: Optional[Dict[str, float]] = None
    evaluation_summary: Optional[Dict[str, Any]] = None
    evaluation_details: Optional[List[Dict[str, Any]]] = None
```

### Agent Initialization

```python
class Agent():
    def __init__(self, purple_agent_url, mcp_server_url, max_num_prob=None, use_cache=True):
        # ... existing setup ...
        
        # Initialize evaluation system with config from file or defaults
        eval_config = load_evaluation_config()
        self.evaluation_pipeline = EvaluationPipeline(eval_config)
        self.metrics_aggregator = MetricsAggregator(eval_config)
        
        print(f"âœ… Evaluation system initialized with {self.evaluation_pipeline.get_evaluator_count()['total']} evaluators")
```

### Evaluation Workflow

```python
async def run(self, message, updater):
    for idx, data in enumerate(test_data[:limit]):
        # 1. Get code from purple agent (with caching)
        # 2. Compile and run code
        # 3. NEW: Evaluate code
        
        if generated_codes:
            await self._evaluate_code(benchmark_result, data, generated_codes)
        
        # 4. Update summary with tier distribution
        if br.tier:
            summary["tier_distribution"][br.tier] += 1
    
    # 5. Generate comprehensive evaluation report
    await self._create_evaluation_report(results, summary, updater)
```

### Private Evaluation Method

```python
async def _evaluate_code(
    self,
    benchmark_result: BenchmarkResult,
    problem_data: Dict[str, Any],
    generated_codes: List[str],
) -> None:
    """Run evaluation pipeline on generated codes."""
    
    # Prepare execution result
    execution_result = {
        'compiles': benchmark_result.compiles,
        'runs': benchmark_result.runs,
        'stdout': benchmark_result.stdout or '',
        'stderr': benchmark_result.stderr or '',
        'execution_time_sec': benchmark_result.time_used_sec,
        'memory_mb': None,
    }
    
    # Run evaluation pipeline
    eval_results = await self.evaluation_pipeline.evaluate(
        code=generated_codes[0],
        problem=problem_data,
        execution_result=execution_result
    )
    
    # Aggregate results
    aggregated = self.metrics_aggregator.aggregate(eval_results)
    
    # Update benchmark result
    benchmark_result.composite_score = aggregated.composite_score
    benchmark_result.tier = aggregated.overall_tier
    benchmark_result.category_scores = {
        'correctness': aggregated.category_scores.correctness,
        'performance': aggregated.category_scores.performance,
        'code_quality': aggregated.category_scores.code_quality,
        'algorithm': aggregated.category_scores.algorithm,
        'petsc': aggregated.category_scores.petsc,
    }
```

## Configuration System

### YAML Configuration (config/evaluation_config.yaml)

```yaml
evaluation:
  # Enable/disable evaluation phases
  enable_gates: true
  enable_metrics: true
  enable_quality: true
  
  # LLM settings for quality evaluators
  llm:
    model: "openai/gpt-4o-mini"
    temperature: 0.3
    max_concurrent_calls: 3  # Rate limiting
  
  # Performance settings
  parallel_evaluation: true
  
  # LLM Thresholds
  thresholds:
    min_llm_confidence: 0.7

# Scoring configuration
scoring:
  # Category weights (must sum to 1.0)
  weights:
    correctness: 0.35
    performance: 0.15
    code_quality: 0.15
    algorithm: 0.15
    petsc: 0.10
    semantic: 0.10
  
  # Tier thresholds (0-100 scale)
  tiers:
    gold: 85
    silver: 70
    bronze: 50
```

### Configuration Loading

Supports multiple formats with graceful fallback:

```python
def load_evaluation_config(config_path: str = "config/evaluation_config.yaml") -> Dict[str, Any]:
    """Load evaluation configuration from file or use defaults.
    
    Supports both JSON and YAML formats. Format is auto-detected by file extension.
    Falls back to sensible defaults if config file not found.
    """
    config_file = Path(config_path)
    
    if config_file.exists():
        try:
            with open(config_file, 'r') as f:
                if config_file.suffix.lower() in ['.yaml', '.yml']:
                    import yaml
                    config_data = yaml.safe_load(f)
                else:
                    config_data = json.load(f)
            
            print(f"âœ… Loaded evaluation config from {config_path}")
            return config_data
        except Exception as e:
            print(f"âš ï¸ Failed to load config: {e}")
    
    # Fall back to defaults
    return { /* default config */ }
```

## Output Format

### Console Output During Evaluation

```
@@@ Green agent: âœ… Evaluation system initialized with 14 evaluators
[1/3] Running Advection_PDE...
@@@ Green agent: âœ… Loaded cached response for Advection_PDE
@@@ Green agent: Compile and run the code...
@@@ Green agent: Evaluating generated code...
Phase 1: Running gate evaluators...
Phase 2: Running metric evaluators...
Phase 3: Running quality evaluators...
Evaluation complete: 14 evaluators ran
@@@ Green agent: âœ… Evaluation complete: Score=87.5, Tier=GOLD
```

### Text Report (evaluation_report.txt)

```
================================================================================
EVALUATION REPORT
================================================================================

Total Problems: 3
Successful Executions: 3
Failed Executions: 0
Average Execution Time: 2.45s

Average Composite Score: 76.3/100

Tier Distribution:
  ğŸ¥‡ GOLD:   1 (33.3%)
  ğŸ¥ˆ SILVER: 1 (33.3%)
  ğŸ¥‰ BRONZE: 1 (33.3%)
  âŒ FAIL:   0 (0.0%)

================================================================================
PER-PROBLEM RESULTS
================================================================================

ğŸ¥‡ Advection_PDE (Score: 87.5/100)
   Correctness: 92.0, Performance: 85.0, Code Quality: 78.0

ğŸ¥ˆ Robertson_ODE (Score: 73.2/100)
   Correctness: 80.0, Performance: 70.0, Code Quality: 68.0

ğŸ¥‰ Rosenbrock_banana_function (Score: 68.1/100)
   Correctness: 75.0, Performance: 65.0, Code Quality: 60.0
```

### JSON Output (output/benchmark_summary.json)

```json
{
  "summary": {
    "total": 3,
    "runs_count": 3,
    "failure_count": 0,
    "avg_time_sec": 2.45,
    "avg_composite_score": 76.3,
    "tier_distribution": {
      "GOLD": 1,
      "SILVER": 1,
      "BRONZE": 1,
      "FAIL": 0
    }
  },
  "results": [
    {
      "problem_name": "Advection_PDE",
      "problem_id": "adv_001",
      "runs": true,
      "time_used_sec": 2.1,
      "compiles": true,
      "composite_score": 87.5,
      "tier": "GOLD",
      "category_scores": {
        "correctness": 92.0,
        "performance": 85.0,
        "code_quality": 78.0,
        "algorithm": 88.0,
        "petsc": 82.0
      },
      "evaluation_summary": {
        "total_evaluators": 14,
        "passed_evaluators": 13,
        "failed_evaluators": 1,
        "all_gates_passed": true,
        "gates_passed": 4,
        "gates_total": 4
      },
      "evaluation_details": [
        {
          "name": "compilation",
          "type": "gate",
          "method": "deterministic",
          "passed": true,
          "score": null,
          "raw_value": null,
          "confidence": 1.0,
          "feedback": "Code compiled successfully"
        },
        {
          "name": "numerical_accuracy",
          "type": "metric",
          "method": "deterministic",
          "passed": null,
          "score": 0.95,
          "raw_value": 1.2e-8,
          "confidence": 1.0,
          "feedback": "Excellent numerical accuracy"
        }
      ]
    }
  ]
}
```

### Detailed Evaluation Report (evaluation_detailed_report.json)

```json
{
  "summary": {
    "total": 3,
    "avg_composite_score": 76.3,
    "tier_distribution": { "GOLD": 1, "SILVER": 1, "BRONZE": 1, "FAIL": 0 }
  },
  "per_problem_scores": [
    {
      "problem_name": "Advection_PDE",
      "problem_id": "adv_001",
      "tier": "GOLD",
      "composite_score": 87.5,
      "category_scores": {
        "correctness": 92.0,
        "performance": 85.0,
        "code_quality": 78.0,
        "algorithm": 88.0,
        "petsc": 82.0
      },
      "evaluation_summary": {
        "total_evaluators": 14,
        "passed_evaluators": 13,
        "failed_evaluators": 1,
        "all_gates_passed": true
      }
    }
  ]
}
```

## Purple Agent Caching System

The Green Agent includes a **caching system** to avoid redundant Purple Agent calls:

```python
def __init__(self, purple_agent_url, mcp_server_url, max_num_prob=None, use_cache=True):
    self.use_cache = use_cache
    self.cache_dir = Path("./purple_agent_cache")
    self.cache_dir.mkdir(exist_ok=True)

def _get_cache_path(self, problem_name: str) -> Path:
    """Get the cache file path for a given problem."""
    safe_name = re.sub(r'[^\w\-_]', '_', problem_name)
    return self.cache_dir / f"{safe_name}.pkl"

def _load_cached_response(self, problem_name: str):
    """Load cached purple agent response if it exists."""
    cache_path = self._get_cache_path(problem_name)
    if cache_path.exists():
        with open(cache_path, 'rb') as f:
            return pickle.load(f)
    return None

def _save_cached_response(self, problem_name: str, response):
    """Save purple agent response to cache."""
    cache_path = self._get_cache_path(problem_name)
    with open(cache_path, 'wb') as f:
        pickle.dump(response, f)
```

**Benefits:**
- ğŸš€ Faster re-evaluation during development
- ğŸ’° No redundant Purple Agent calls (cost savings)
- ğŸ”„ Consistent results for testing evaluation changes
- ğŸ“ Stored in `./purple_agent_cache/` as `.pkl` files

## Performance & Cost

### Evaluation Time

| Configuration | Time | Cost (per problem) |
|---------------|------|--------------------| 
| Gates + Metrics only | ~200ms | $0.00 |
| + Quality (static) | ~500ms | $0.00 |
| + Quality (LLM mini) | ~30-45s | ~$0.01-0.02 |
| + Quality (LLM gpt-4o) | ~30-45s | ~$0.10-0.15 |

### Optimization Strategies

1. **Development/Testing**: 
   ```yaml
   enable_quality: false  # Skip LLM evaluations
   ```

2. **Production**: 
   ```yaml
   llm:
     model: "openai/gpt-4o-mini"  # Good quality/cost ratio
   ```

3. **Research/High Quality**: 
   ```yaml
   llm:
     model: "openai/gpt-4o"  # Best quality
   ```

4. **Hybrid Approach**: 
   ```yaml
   readability:
     use_llm: false  # Use static analysis
   code_style:
     use_llm: false
   # Keep LLM for algorithm quality
   ```

## Usage Examples

### Running the Full Benchmark

```bash
# Run with evaluation enabled (default)
python main.py

# Or use the launcher
python src/launcher.py
```

### Quick Test Without LLM

Edit `config/evaluation_config.yaml`:
```yaml
evaluation:
  enable_quality: false  # Fast mode
```

### Standalone Evaluation

```python
from src.evaluators import EvaluationPipeline
from src.metrics import MetricsAggregator
from src.green_agent.agent import load_evaluation_config

# Initialize
config = load_evaluation_config()
pipeline = EvaluationPipeline(config)
aggregator = MetricsAggregator(config)

# Prepare execution result
execution_result = {
    'compiles': True,
    'runs': True,
    'stdout': '...',
    'execution_time_sec': 2.5,
}

# Evaluate
results = await pipeline.evaluate(code, problem_data, execution_result)
metrics = aggregator.aggregate(results)

print(f"Score: {metrics.composite_score:.1f}/100")
print(f"Tier: {metrics.overall_tier}")
```

### Custom Evaluator

```python
from src.evaluators.base import Evaluator, EvaluatorType, EvaluationResult

class CustomQualityEvaluator(Evaluator):
    @property
    def name(self) -> str:
        return "custom_metric"
    
    @property
    def evaluator_type(self) -> EvaluatorType:
        return EvaluatorType.QUALITY
    
    async def evaluate(self, code, problem, execution_result):
        # Your evaluation logic here
        score = 0.85  # 0-1 scale
        
        return EvaluationResult(
            evaluator_name=self.name,
            evaluator_type=self.evaluator_type,
            quality_score=score,
            confidence=0.9,
            feedback="Custom evaluation passed",
            evaluation_method="custom"
        )

# Add to pipeline
pipeline.add_evaluator(CustomQualityEvaluator())
```

## Files in Repository (Complete Implementation)

### Evaluators (14 files)

```
src/evaluators/
â”œâ”€â”€ __init__.py                          # âœ… Exports
â”œâ”€â”€ base.py                              # âœ… Base classes, EvaluatorType enum
â”œâ”€â”€ pipeline.py                          # âœ… Orchestration
â”œâ”€â”€ README.md                            # âœ… Documentation
â”œâ”€â”€ gates/                               # âœ… 4 gate evaluators
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ compilation_gate.py
â”‚   â”œâ”€â”€ execution_gate.py
â”‚   â”œâ”€â”€ memory_safety_gate.py
â”‚   â””â”€â”€ api_usage_gate.py
â”œâ”€â”€ metrics/                             # âœ… 2 metric evaluators
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ numerical_accuracy.py
â”‚   â””â”€â”€ execution_time.py
â””â”€â”€ quality/                             # âœ… 8 quality evaluators
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ code_quality/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ readability.py
    â”‚   â”œâ”€â”€ code_style.py
    â”‚   â””â”€â”€ documentation.py
    â”œâ”€â”€ algorithm_quality/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ algorithm_appropriateness.py
    â”‚   â””â”€â”€ solver_choice.py
    â””â”€â”€ petsc_quality/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ best_practices.py
        â”œâ”€â”€ error_handling.py
        â””â”€â”€ parallel_awareness.py
```

### Supporting Infrastructure

```
src/metrics/
â”œâ”€â”€ __init__.py                          # âœ… Exports
â”œâ”€â”€ types.py                             # âœ… AggregatedMetrics, CategoryScores
â””â”€â”€ aggregation.py                       # âœ… MetricsAggregator

src/util/
â””â”€â”€ llm_client.py                        # âœ… OpenAI wrapper

src/green_agent/
â””â”€â”€ agent.py                             # âœ… FULLY INTEGRATED

config/
â””â”€â”€ evaluation_config.yaml               # âœ… Configuration

examples/
â””â”€â”€ evaluation_example.py                # âœ… Usage example
```

### Generated Output

```
output/
â”œâ”€â”€ benchmark_summary.json               # Main results with evaluations
â”œâ”€â”€ benchmark_result_{problem}.json      # Per-problem detailed results
â”œâ”€â”€ evaluation_report.txt                # Human-readable summary
â””â”€â”€ evaluation_detailed_report.json      # Detailed evaluation breakdown

purple_agent_cache/
â”œâ”€â”€ Advection_PDE.pkl                    # Cached responses
â”œâ”€â”€ Robertson_ODE.pkl
â””â”€â”€ Rosenbrock_banana_function.pkl

generated_codes/
â”œâ”€â”€ Advection_PDE.c                      # Generated code files
â”œâ”€â”€ Robertson_ODE.c
â””â”€â”€ Rosenbrock_banana_function.c
```

## Production Status

âœ… **Complete implementation** with all 14 evaluators

âœ… **Fully integrated** with Green Agent benchmarking

âœ… **Configurable** via YAML/JSON

âœ… **Well-documented** with README and examples

âœ… **Tested** on real PETSc problems

âœ… **Caching** for efficient development

âœ… **Comprehensive reporting** with multiple output formats

âœ… **Production-ready** and actively running
