# PETSc Agent Benchmark

An agentified evaluation framework for testing PETSc code generation agents using A2A (Agent-to-Agent) and MCP (Model Context Protocol) standards.

## Overview

This project implements a multi-agent system for evaluating code generation agents that produce PETSc (Portable, Extensible Toolkit for Scientific Computation) programs. The system uses:

- **A2A Protocol**: Standardized agent-to-agent communication
- **MCP Protocol**: Tool access for compilation and execution
- **Multi-tier Evaluation**: Gates, metrics, and quality assessments
- **Automated Benchmarking**: End-to-end evaluation workflow

## Architecture

The system consists of three main components:

1. **Green Agent** (Assessment Manager)
   - Loads benchmark problems from the dataset
   - Distributes tasks to the Purple Agent
   - Evaluates generated code through a comprehensive pipeline
   - Generates detailed assessment reports

2. **Purple Agent** (Target Under Test)
   - Receives problem descriptions via A2A
   - Generates PETSc C/C++ code using an LLM
   - Returns code and CLI arguments
   - Isolated from evaluation logic

3. **MCP Server** (Tool Provider)
   - Provides compilation tools (make)
   - Provides execution tools (run with arguments)
   - Manages PETSc environment configuration

## Project Structure

```
â”œâ”€â”€ data/                           # Benchmark problem datasets
â”‚   â””â”€â”€ problems_test.jsonl         # Test problem specifications
â”œâ”€â”€ main.py                         # CLI entry point
â”œâ”€â”€ pyproject.toml                  # Python project configuration
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ green_agent/                # Assessment manager agent
â”‚   â”‚   â”œâ”€â”€ agent.py               # Core evaluation logic
â”‚   â”‚   â”œâ”€â”€ server.py              # A2A server implementation
â”‚   â”‚   â””â”€â”€ mcp_client.py          # MCP client for tools
â”‚   â”œâ”€â”€ purple_agent/               # Target agent being tested
â”‚   â”‚   â””â”€â”€ petsc_agent.py         # Code generation agent
â”‚   â”œâ”€â”€ evaluators/                 # Evaluation system
â”‚   â”‚   â”œâ”€â”€ base.py                # Base evaluator classes
â”‚   â”‚   â”œâ”€â”€ pipeline.py            # Evaluation orchestrator
â”‚   â”‚   â”œâ”€â”€ gates/                 # Binary pass/fail checks
â”‚   â”‚   â”œâ”€â”€ metrics/               # Quantitative measurements
â”‚   â”‚   â””â”€â”€ quality/               # Quality assessments
â”‚   â”œâ”€â”€ metrics/                    # Metrics aggregation
â”‚   â”‚   â”œâ”€â”€ aggregation.py         # Score computation
â”‚   â”‚   â””â”€â”€ types.py               # Metric data types
â”‚   â”œâ”€â”€ util/                       # Utility modules
â”‚   â”‚   â”œâ”€â”€ a2a_comm.py            # A2A communication helpers
â”‚   â”‚   â””â”€â”€ llm_client.py          # LLM client utilities
â”‚   â””â”€â”€ launcher.py                # Evaluation coordinator
â”œâ”€â”€ config/                         # Configuration files
â”‚   â””â”€â”€ evaluation_config.yaml     # Evaluation pipeline config
â”œâ”€â”€ output/                         # Generated reports and results
â”œâ”€â”€ generated_codes/                # Code generated by Purple Agent
â””â”€â”€ purple_agent_cache/             # Cached responses (optional)
```

## Installation

### Prerequisites

1. **PETSc Installation**: Install PETSc from [https://petsc.org/](https://petsc.org/)
2. **Python 3.11+**: Required for the evaluation framework
3. **UV Package Manager**: Install from [https://github.com/astral-sh/uv](https://github.com/astral-sh/uv)

### Setup

1. Install dependencies using `uv`:

```bash
uv sync
```

2. Create a `.env` file in the root directory with the following variables:

```bash
# LLM API Keys (at least one required)
GEMINI_API_KEY="<your_gemini_key>"
OPENAI_API_KEY="<your_openai_key>"

# PETSc Configuration (required for compilation/execution)
PETSC_DIR="<path_to_petsc_installation>"
PETSC_ARCH="<petsc_architecture>"  # e.g., arch-darwin-c-debug
```

## Usage

### Quick Start

Launch the complete evaluation workflow:

```bash
uv run main.py launch
```

This command will:
1. Start the Green Agent (assessment manager)
2. Start the Purple Agent (code generator)
3. Start the MCP server (compilation/execution tools)
4. Run all benchmark problems
5. Generate evaluation reports in `output/`

### Individual Components

You can also run components separately for development/debugging:

```bash
# Start only the Green Agent
uv run main.py green

# Start only the Purple Agent
uv run main.py purple
```

### Configuration

The evaluation pipeline can be configured via `config/evaluation_config.yaml`:

```yaml
evaluation:
  enable_gates: true          # Enable binary pass/fail checks
  enable_metrics: true        # Enable quantitative measurements
  enable_quality: true        # Enable quality assessments
  parallel_evaluation: true   # Run evaluators in parallel
  
  llm:
    model: "openai/gpt-4o-mini"  # LLM for quality evaluation
    temperature: 0.3             # LLM temperature
    max_concurrent_calls: 3      # Rate limiting for LLM calls
  
  thresholds:
    min_llm_confidence: 0.7      # Minimum confidence for LLM evaluations

scoring:
  weights:
    correctness: 0.35     # Weight for correctness score
    performance: 0.15     # Weight for performance metrics
    code_quality: 0.15    # Weight for code quality
    algorithm: 0.15       # Weight for algorithm choice
    petsc: 0.10          # Weight for PETSc best practices
    semantic: 0.10       # Weight for semantic correctness
  
  tiers:
    gold: 85      # Minimum score for GOLD tier
    silver: 70    # Minimum score for SILVER tier
    bronze: 50    # Minimum score for BRONZE tier
```

## Evaluation System

The evaluation pipeline consists of three phases:

### Phase 1: Gates (Must Pass)

Binary checks that code must pass to be considered valid:

- **Compilation Gate**: Code must compile without errors
- **Execution Gate**: Code must run without crashes
- **Memory Safety Gate**: No memory leaks or invalid accesses
- **API Usage Gate**: Correct PETSc API usage

If any gate fails, evaluation stops and the code receives a FAIL tier.

### Phase 2: Metrics (Measurements)

Quantitative measurements of code performance:

- **Numerical Accuracy**: Correctness of numerical results
- **Execution Time**: Runtime performance

### Phase 3: Quality (Assessments)

Qualitative assessments of code quality:

**Code Quality:**
- Readability: Code structure and naming
- Code Style: Adherence to C/C++ conventions
- Documentation: Comments and explanations

**Algorithm Quality:**
- Algorithm Appropriateness: Suitability for the problem
- Solver Choice: Optimal PETSc solver selection

**PETSc Quality:**
- Best Practices: PETSc usage patterns
- Error Handling: Proper error checking
- Parallel Awareness: MPI and parallel considerations

## Output

Evaluation results are saved to the `output/` directory:

- `benchmark_summary.json`: Overall statistics and per-problem results
- `evaluation_report.txt`: Human-readable summary report
- `evaluation_detailed_report.json`: Detailed scores and feedback
- `benchmark_result_<problem_name>.json`: Individual problem results

### Tier System

Codes are assigned to tiers based on composite scores:

- ğŸ¥‡ **GOLD** (â‰¥85): Excellent code quality and correctness
- ğŸ¥ˆ **SILVER** (â‰¥70): Good code with minor issues
- ğŸ¥‰ **BRONZE** (â‰¥50): Functional but needs improvement
- âŒ **FAIL** (<50 or gate failure): Significant issues

## Development

### Adding Custom Evaluators

To add a new evaluator:

1. Create a new evaluator class inheriting from `Evaluator`
2. Implement required properties: `name`, `evaluator_type`, `evaluation_method`
3. Implement the `evaluate()` method
4. Add to the pipeline in `src/evaluators/pipeline.py`

Example:

```python
from src.evaluators.base import Evaluator, EvaluatorType, EvaluationResult

class MyCustomEvaluator(Evaluator):
    @property
    def name(self) -> str:
        return "my_custom_check"
    
    @property
    def evaluator_type(self) -> EvaluatorType:
        return EvaluatorType.QUALITY
    
    async def evaluate(self, code, problem, execution_result):
        # Your evaluation logic here
        return EvaluationResult(
            evaluator_name=self.name,
            evaluator_type=self.evaluator_type,
            quality_score=0.8,
            feedback="Custom evaluation passed"
        )
```

### Caching

The Green Agent supports caching Purple Agent responses to speed up development:

```python
# In src/green_agent/agent.py
agent = Agent(
    purple_agent_url=purple_url,
    mcp_server_url=mcp_url,
    use_cache=True  # Enable caching
)
```

Cached responses are stored in `purple_agent_cache/`.

## Troubleshooting

### Common Issues

1. **PETSc not found**: Ensure `PETSC_DIR` and `PETSC_ARCH` are set correctly in `.env`
2. **LLM API errors**: Verify API keys are valid and have sufficient quota
3. **Agent timeout**: Increase timeout in `src/util/a2a_comm.py` if needed
4. **Port conflicts**: Modify ports in `src/launcher.py` if defaults are in use

### Logs

Check log files for detailed error information:
- `petsc_compile_run_mcp_server.log`: MCP server logs
- Console output from Green/Purple agents

## Contributing

Contributions are welcome! Please:

1. Follow existing code style and documentation patterns
2. Add docstrings to all functions and classes
3. Include type hints where applicable
4. Test changes with the full evaluation pipeline

## License

See LICENSE file for details.
